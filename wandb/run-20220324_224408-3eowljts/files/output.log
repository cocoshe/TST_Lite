
| epoch   1 |    10/   90 batches | lr 0.001000 | 131.95 ms | loss 0.17088
| epoch   1 |    20/   90 batches | lr 0.001000 | 118.01 ms | loss 0.15822
| epoch   1 |    30/   90 batches | lr 0.001000 | 121.53 ms | loss 0.09970
| epoch   1 |    40/   90 batches | lr 0.001000 | 139.81 ms | loss 0.08955
| epoch   1 |    50/   90 batches | lr 0.001000 | 115.29 ms | loss 0.07580
| epoch   1 |    60/   90 batches | lr 0.001000 | 113.76 ms | loss 0.06037
| epoch   1 |    70/   90 batches | lr 0.001000 | 118.92 ms | loss 0.08759
| epoch   1 |    80/   90 batches | lr 0.001000 | 115.92 ms | loss 0.07624
| epoch   1 |    90/   90 batches | lr 0.001000 | 111.84 ms | loss 0.11296
data_source shape: torch.Size([5784, 2, 20, 36])
output shape: torch.Size([20, 1, 36])
output[[0]].shape: torch.Size([1, 1, 36])
output[:-1].shape: torch.Size([19, 1, 36])
truth shape: torch.Size([5803, 36])
test_result shape: torch.Size([5803, 36])
output truth shape: (5803, 36)
output test_result shape: (5803, 36)
test_result[0].shape, type (36,) float64
test_result.shape, type (5803, 36) float64
loss shape:  (5803, 36)
output_df shape: (5803, 108)
---------------------------------
start SVM
---------------------------------
精度为0.9893177119228118
precision:  1.0  recall:  0.5633802816901409  acc:  0.9946579355505772  f1:  0.7207207207207207
混淆矩阵:
 [[5732    0]
 [  31   40]]
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 41.74s | valid loss 8.31919
-----------------------------------------------------------------------------------------
save successfully
| epoch   2 |    10/   90 batches | lr 0.001000 | 121.68 ms | loss 0.06015
| epoch   2 |    20/   90 batches | lr 0.001000 | 120.91 ms | loss 0.06488
| epoch   2 |    30/   90 batches | lr 0.001000 | 116.76 ms | loss 0.04960
| epoch   2 |    40/   90 batches | lr 0.001000 | 118.13 ms | loss 0.05019
| epoch   2 |    50/   90 batches | lr 0.001000 | 115.33 ms | loss 0.04452
| epoch   2 |    60/   90 batches | lr 0.001000 | 117.11 ms | loss 0.03720
| epoch   2 |    70/   90 batches | lr 0.001000 | 114.81 ms | loss 0.06934
| epoch   2 |    80/   90 batches | lr 0.001000 | 116.74 ms | loss 0.06268
| epoch   2 |    90/   90 batches | lr 0.001000 | 109.19 ms | loss 0.09061
data_source shape: torch.Size([5784, 2, 20, 36])
output shape: torch.Size([20, 1, 36])
output[[0]].shape: torch.Size([1, 1, 36])
output[:-1].shape: torch.Size([19, 1, 36])
truth shape: torch.Size([5803, 36])
test_result shape: torch.Size([5803, 36])
output truth shape: (5803, 36)
output test_result shape: (5803, 36)
test_result[0].shape, type (36,) float64
test_result.shape, type (5803, 36) float64
loss shape:  (5803, 36)
output_df shape: (5803, 108)
---------------------------------
start SVM
---------------------------------
精度为0.9896623018607856
precision:  1.0  recall:  0.5774647887323944  acc:  0.9948302602102361  f1:  0.7321428571428571
混淆矩阵:
 [[5732    0]
 [  30   41]]
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 40.20s | valid loss 7.97860
-----------------------------------------------------------------------------------------
save successfully
| epoch   3 |    10/   90 batches | lr 0.001000 | 126.85 ms | loss 0.04954
| epoch   3 |    20/   90 batches | lr 0.001000 | 116.00 ms | loss 0.05342
| epoch   3 |    30/   90 batches | lr 0.001000 | 114.91 ms | loss 0.03893
| epoch   3 |    40/   90 batches | lr 0.001000 | 115.23 ms | loss 0.03972
| epoch   3 |    50/   90 batches | lr 0.001000 | 118.23 ms | loss 0.03639
| epoch   3 |    60/   90 batches | lr 0.001000 | 117.50 ms | loss 0.03022
| epoch   3 |    70/   90 batches | lr 0.001000 | 117.26 ms | loss 0.06496
| epoch   3 |    80/   90 batches | lr 0.001000 | 114.99 ms | loss 0.05796
| epoch   3 |    90/   90 batches | lr 0.001000 | 110.01 ms | loss 0.08235
data_source shape: torch.Size([5784, 2, 20, 36])
output shape: torch.Size([20, 1, 36])
output[[0]].shape: torch.Size([1, 1, 36])
output[:-1].shape: torch.Size([19, 1, 36])
truth shape: torch.Size([5803, 36])
test_result shape: torch.Size([5803, 36])
output truth shape: (5803, 36)
output test_result shape: (5803, 36)
test_result[0].shape, type (36,) float64
test_result.shape, type (5803, 36) float64
loss shape:  (5803, 36)
output_df shape: (5803, 108)
---------------------------------
start SVM
---------------------------------
精度为0.9920744314266023
precision:  1.0  recall:  0.676056338028169  acc:  0.9960365328278477  f1:  0.8067226890756303
混淆矩阵:
 [[5732    0]
 [  23   48]]
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 39.12s | valid loss 7.43278
-----------------------------------------------------------------------------------------
save successfully
| epoch   4 |    10/   90 batches | lr 0.001000 | 124.45 ms | loss 0.04497
| epoch   4 |    20/   90 batches | lr 0.001000 | 115.15 ms | loss 0.04987
| epoch   4 |    30/   90 batches | lr 0.001000 | 114.49 ms | loss 0.03586
| epoch   4 |    40/   90 batches | lr 0.001000 | 114.47 ms | loss 0.03552
| epoch   4 |    50/   90 batches | lr 0.001000 | 113.79 ms | loss 0.03331
| epoch   4 |    60/   90 batches | lr 0.001000 | 116.24 ms | loss 0.02740
| epoch   4 |    70/   90 batches | lr 0.001000 | 115.77 ms | loss 0.06264
| epoch   4 |    80/   90 batches | lr 0.001000 | 118.59 ms | loss 0.05624
| epoch   4 |    90/   90 batches | lr 0.001000 | 110.84 ms | loss 0.07880
data_source shape: torch.Size([5784, 2, 20, 36])
output shape: torch.Size([20, 1, 36])
output[[0]].shape: torch.Size([1, 1, 36])
output[:-1].shape: torch.Size([19, 1, 36])
truth shape: torch.Size([5803, 36])
test_result shape: torch.Size([5803, 36])
output truth shape: (5803, 36)
output test_result shape: (5803, 36)
test_result[0].shape, type (36,) float64
test_result.shape, type (5803, 36) float64
loss shape:  (5803, 36)
output_df shape: (5803, 108)
---------------------------------
start SVM
---------------------------------
精度为0.9875947622329428
precision:  1.0  recall:  0.49295774647887325  acc:  0.9937963122522833  f1:  0.660377358490566
混淆矩阵:
 [[5732    0]
 [  36   35]]
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 44.14s | valid loss 6.92690
-----------------------------------------------------------------------------------------
save successfully
| epoch   5 |    10/   90 batches | lr 0.001000 | 125.50 ms | loss 0.04222
| epoch   5 |    20/   90 batches | lr 0.001000 | 113.41 ms | loss 0.04712
| epoch   5 |    30/   90 batches | lr 0.001000 | 113.29 ms | loss 0.03377
| epoch   5 |    40/   90 batches | lr 0.001000 | 117.19 ms | loss 0.03316
| epoch   5 |    50/   90 batches | lr 0.001000 | 119.97 ms | loss 0.03149
| epoch   5 |    60/   90 batches | lr 0.001000 | 118.16 ms | loss 0.02539
| epoch   5 |    70/   90 batches | lr 0.001000 | 118.13 ms | loss 0.06061
| epoch   5 |    80/   90 batches | lr 0.001000 | 115.84 ms | loss 0.05441
| epoch   5 |    90/   90 batches | lr 0.001000 | 107.66 ms | loss 0.07816
data_source shape: torch.Size([5784, 2, 20, 36])
output shape: torch.Size([20, 1, 36])
output[[0]].shape: torch.Size([1, 1, 36])
output[:-1].shape: torch.Size([19, 1, 36])
truth shape: torch.Size([5803, 36])
test_result shape: torch.Size([5803, 36])
output truth shape: (5803, 36)
output test_result shape: (5803, 36)
test_result[0].shape, type (36,) float64
test_result.shape, type (5803, 36) float64
loss shape:  (5803, 36)
output_df shape: (5803, 108)
---------------------------------
start SVM
---------------------------------
精度为0.9875947622329428
precision:  1.0  recall:  0.49295774647887325  acc:  0.9937963122522833  f1:  0.660377358490566
混淆矩阵:
 [[5732    0]
 [  36   35]]
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 44.12s | valid loss 6.83810
-----------------------------------------------------------------------------------------
save successfully
| epoch   6 |    10/   90 batches | lr 0.001000 | 124.71 ms | loss 0.04051
| epoch   6 |    20/   90 batches | lr 0.001000 | 121.45 ms | loss 0.04548
| epoch   6 |    30/   90 batches | lr 0.001000 | 147.27 ms | loss 0.03288
| epoch   6 |    40/   90 batches | lr 0.001000 | 125.03 ms | loss 0.03229
Traceback (most recent call last):
  File "train.py", line 150, in <module>
    main(args)
  File "train.py", line 111, in main
    train(train_data, input_window, model, optimizer, criterion, scheduler, epoch, batch_size)
  File "D:\PythonProject\Time_series_anomaly_detection\utils\train.py", line 17, in train
    output = model(data)
  File "D:\miniconda\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\PythonProject\Time_series_anomaly_detection\model\Transformer.py", line 48, in forward
    output = self.transformer_encoder(src, self.src_mask)  # , self.src_mask)
  File "D:\miniconda\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\miniconda\envs\pytorch\lib\site-packages\torch\nn\modules\transformer.py", line 198, in forward
    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)
  File "D:\miniconda\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\miniconda\envs\pytorch\lib\site-packages\torch\nn\modules\transformer.py", line 340, in forward
    x = self.norm2(x + self._ff_block(x))
  File "D:\miniconda\envs\pytorch\lib\site-packages\torch\nn\modules\transformer.py", line 355, in _ff_block
    x = self.linear2(self.dropout(self.activation(self.linear1(x))))
  File "D:\miniconda\envs\pytorch\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\miniconda\envs\pytorch\lib\site-packages\torch\nn\modules\dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "D:\miniconda\envs\pytorch\lib\site-packages\torch\nn\functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
KeyboardInterrupt