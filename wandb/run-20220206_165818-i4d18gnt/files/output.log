D:\PythonProject\Time_series_anomaly_detection\weights\2022_02_06 16_58_24
series:  0      29.9
1      30.6
2      24.8
3      37.2
4      21.0
       ...
304    41.7
305    35.3
306     0.0
307    39.8
308    41.9
Name: concentration, Length: 309, dtype: float64
D:\PythonProject\Time_series_anomaly_detection\utils\data_prepare.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\torch\csrc\utils\tensor_new.cpp:201.)
  return torch.FloatTensor(inout_seq)
-----------------------------------------------------------------------------------------
| end of epoch   1 | time:  1.75s | valid loss 0.08220
-----------------------------------------------------------------------------------------
save successfully
-----------------------------------------------------------------------------------------
| end of epoch   2 | time:  1.72s | valid loss 0.21491
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   3 | time:  1.74s | valid loss 0.03623
-----------------------------------------------------------------------------------------
save successfully
-----------------------------------------------------------------------------------------
| end of epoch   4 | time:  1.69s | valid loss 0.06474
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   5 | time:  1.75s | valid loss 0.04908
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   6 | time:  1.72s | valid loss 0.04021
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   7 | time:  1.68s | valid loss 0.04224
-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   8 | time:  1.76s | valid loss 0.03020
-----------------------------------------------------------------------------------------
save successfully
-----------------------------------------------------------------------------------------
| end of epoch   9 | time:  1.75s | valid loss 0.02656
-----------------------------------------------------------------------------------------
save successfully
Traceback (most recent call last):
  File "train.py", line 138, in <module>
    main(args)
  File "train.py", line 103, in main
    val_loss = plot_and_loss(model, val_data, epoch, criterion, input_window, timestamp, scaler)
  File "D:\PythonProject\Time_series_anomaly_detection\utils\plot_and_loss.py", line 30, in plot_and_loss
    truth = scaler.inverse_transform(truth)
  File "D:\miniconda\envs\pytorch\lib\site-packages\sklearn\preprocessing\_data.py", line 526, in inverse_transform
    X, copy=self.copy, dtype=FLOAT_DTYPES, force_all_finite="allow-nan"
  File "D:\miniconda\envs\pytorch\lib\site-packages\sklearn\utils\validation.py", line 773, in check_array
    "if it contains a single sample.".format(array)
ValueError: Expected 2D array, got 1D array instead:
array=[-0.80527514 -0.8007164  -0.83848912 -0.75773364 -0.86323673 -0.81569523
 -0.82155651 -0.80788016 -0.82481277 -0.83523279 -0.72777599 -0.86649299
 -0.82611525 -0.8046239  -0.74275482 -0.85998046 -0.85151416 -0.78704005
 -0.83458155 -0.85542166 -0.77206123 -0.8567242  -0.85607296 -0.86323673
 -0.85607296 -0.80657768 -0.75056982 -0.82806903 -0.80332142 -0.79029632
 -0.77987623 -0.83393031 -0.77596873 -0.85411918 -0.79746014 -0.82351023
 -0.75382614 -0.85021168 -0.83783782 -0.82872027 -0.81048518 -0.77987623
 -0.85477042 -0.79159886 -0.8091827  -0.85607296 -0.81309021 -0.83523279
 -0.77336371 -0.82611525 -0.84239662 -0.8443504  -0.83067405 -0.85477042
 -0.81895149 -0.84044284 -0.78248125 -0.83718657 -0.72126341 -0.74796486
 -0.53305113 -0.85542166 -0.84890914 -0.8528167  -0.87495929 -0.87040055
 -0.84174538 -0.78834254 -0.84630412 -0.84565288 -0.83002281 -0.82676655
 -0.78638881 -0.82155651 -0.80983394 -0.8000651  -0.82611525 -0.53630739
 -0.84630412 -0.84890914 -0.84239662 -0.83718657 -0.82285899 -0.84630412
 -0.83718657 -0.82416153 -0.82090527 -0.81830025 -0.82416153 -0.66590685
 -0.80983394 -0.76685119 -0.80397266 -0.75056982 -0.84239662 -0.83132529
 -0.74340606 -0.82155651 -0.76945621 -0.77466625 -0.76750243 -0.80527514
 -0.81830025 -0.47769457 -0.85477042 -0.84109408 -0.8397916  -0.85021168
 -0.82416153 -0.83067405 -0.81309021 -0.82611525 -0.80201888 -0.84239662
 -0.83653533 -0.77466625 -0.81830025 -0.81243896 -0.73428851 -0.83588409
 -0.78964508 -0.76945621 -0.83914036 -0.83653533 -0.75382614 -0.74535984
 -0.76489741 -0.67176813 -0.88342559 -0.86519051 -0.8521654  -0.85151416
 -0.83914036 -0.85607296 -0.86193424 -0.77727127 -0.56431133 -0.82220775
 -0.73168349 -0.76750243 -0.70628458 -0.82025397 -0.67958319 -0.6007815
 -0.70693582 -0.37805274 -0.60208404 -0.68153697 -0.62618041 -0.67632693
 -0.66069686 -0.648323   -0.66395313 -0.604689   -0.561055   -0.62878543
 -0.6528818  -0.65223056 -0.69000328 -0.5408662  -0.48225334 -0.6444155
 -0.69716704 -0.70758712  1.         -0.66004556 -0.74145228 -0.74145228
 -0.73949856 -0.70563334 -0.691957   -1.         -0.70823836 -0.70302832
 -0.58254641 -0.64962554 -0.64376426 -0.44513187 -0.58189517 -0.51611853
 -0.37870401 -0.48681211 -0.63855422 -0.58515143 -0.30380982 -0.48355585
 -0.48160207 -0.50700098 -0.49202216 -0.6926083  -0.46922827 -0.52588731
 -0.53109735 -0.61250407 -0.60338652 -0.59622276 -0.6489743  -0.84304786
 -0.77466625 -0.68088573 -0.68609571 -0.67241943 -0.68088573 -0.68870074
 -0.64702052 -0.55584502 -0.55910128 -0.55845004 -0.49853468 -0.62552917
 -0.596874   -0.57668513 -0.61185282 -0.60273528 -0.6528818  -0.5447737
 -0.57017261 -0.56040376 -0.48811463 -1.         -1.         -1.
 -1.         -1.         -1.         -1.         -1.         -1.
 -1.         -0.58450013 -1.         -1.         -0.60403776 -0.63464671
 -0.65223056 -0.64962554 -0.45359817 -0.41191795 -0.64636928 -0.61510909
 -0.50569844 -0.52523607 -0.55519372 -0.58971018 -0.52979487 -0.53174859
 -0.62031913 -0.72126341 -0.68088573 -0.61185282 -0.63725173 -0.65418428
 -0.6880495  -0.66590685 -0.67567569 -0.66590685 -0.76294369 -0.69977206
 -0.7922501  -0.74275482 -0.65157926 -0.62552917 -0.63920546 -0.65223056
 -0.57342887 -0.74666232 -0.62552917 -0.60989904 -0.64702052 -0.65809184
 -0.6965158  -0.75317484 -0.66460437 -0.62357539 -0.63464671 -0.66590685
 -0.6489743  -0.62683165 -0.5617063  -0.56561381 -0.64115924 -0.65483558
 -0.67307067 -0.69912082 -0.69912082 -0.65809184 -0.64246172 -0.64636928
 -0.6613481  -0.60794532 -0.56821883 -0.57929015 -0.58645391 -0.58124387
 -0.58971018 -0.60664278 -0.66330189 -0.70367956 -0.7049821  -0.67502445
 -0.6613481  -0.67762941 -0.6567893  -0.72777599 -0.72842723 -0.77010745
 -1.        ].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.